{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11063822,"sourceType":"datasetVersion","datasetId":6893917},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102},{"sourceId":278325,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":238402,"modelId":260072},{"sourceId":287296,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":246195,"modelId":267794},{"sourceId":332384,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":278619,"modelId":299522}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:16:39.159449Z","iopub.execute_input":"2025-04-19T09:16:39.159771Z","iopub.status.idle":"2025-04-19T09:16:39.562419Z","shell.execute_reply.started":"2025-04-19T09:16:39.159746Z","shell.execute_reply":"2025-04-19T09:16:39.561489Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llama_subtopic/transformers/default/1/adapter_model.safetensors\n/kaggle/input/llama_subtopic/transformers/default/1/training_args.bin\n/kaggle/input/llama_subtopic/transformers/default/1/adapter_config.json\n/kaggle/input/llama_subtopic/transformers/default/1/README.md\n/kaggle/input/llama_subtopic/transformers/default/1/tokenizer.json\n/kaggle/input/llama_subtopic/transformers/default/1/tokenizer_config.json\n/kaggle/input/llama_subtopic/transformers/default/1/special_tokens_map.json\n/kaggle/input/llama_subtopic/transformers/default/1/lora/adapter_model.safetensors\n/kaggle/input/llama_subtopic/transformers/default/1/lora/adapter_config.json\n/kaggle/input/llama_subtopic/transformers/default/1/lora/README.md\n/kaggle/input/passag-topic/passage-topic.csv\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n/kaggle/input/llama_16-1/transformers/v1/1/adapter_model.safetensors\n/kaggle/input/llama_16-1/transformers/v1/1/training_args.bin\n/kaggle/input/llama_16-1/transformers/v1/1/adapter_config.json\n/kaggle/input/llama_16-1/transformers/v1/1/README.md\n/kaggle/input/llama_16-1/transformers/v1/1/tokenizer.json\n/kaggle/input/llama_16-1/transformers/v1/1/tokenizer_config.json\n/kaggle/input/llama_16-1/transformers/v1/1/special_tokens_map.json\n/kaggle/input/llama_16-1/transformers/v1/1/lora/adapter_model.safetensors\n/kaggle/input/llama_16-1/transformers/v1/1/lora/adapter_config.json\n/kaggle/input/llama_16-1/transformers/v1/1/lora/README.md\n/kaggle/input/rephrased_llama_3.1/transformers/v1/1/adapter_model.safetensors\n/kaggle/input/rephrased_llama_3.1/transformers/v1/1/training_args.bin\n/kaggle/input/rephrased_llama_3.1/transformers/v1/1/adapter_config.json\n/kaggle/input/rephrased_llama_3.1/transformers/v1/1/README.md\n/kaggle/input/rephrased_llama_3.1/transformers/v1/1/tokenizer.json\n/kaggle/input/rephrased_llama_3.1/transformers/v1/1/tokenizer_config.json\n/kaggle/input/rephrased_llama_3.1/transformers/v1/1/special_tokens_map.json\n/kaggle/input/rephrased_llama_3.1/transformers/v1/1/lora/adapter_model.safetensors\n/kaggle/input/rephrased_llama_3.1/transformers/v1/1/lora/adapter_config.json\n/kaggle/input/rephrased_llama_3.1/transformers/v1/1/lora/README.md\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:16:39.992537Z","iopub.execute_input":"2025-04-19T09:16:39.992992Z","iopub.status.idle":"2025-04-19T09:16:47.265568Z","shell.execute_reply.started":"2025-04-19T09:16:39.992964Z","shell.execute_reply":"2025-04-19T09:16:47.264537Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install fastapi uvicorn nest-asyncio pyngrok transformers torch bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:16:47.266985Z","iopub.execute_input":"2025-04-19T09:16:47.267303Z","iopub.status.idle":"2025-04-19T09:16:54.730247Z","shell.execute_reply.started":"2025-04-19T09:16:47.267277Z","shell.execute_reply":"2025-04-19T09:16:54.729131Z"}},"outputs":[{"name":"stdout","text":"Collecting fastapi\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nCollecting uvicorn\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\nCollecting pyngrok\n  Downloading pyngrok-7.2.4-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.11.0a2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.29.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.2.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyngrok-7.2.4-py3-none-any.whl (23 kB)\nDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi, bitsandbytes\nSuccessfully installed bitsandbytes-0.45.5 fastapi-0.115.12 pyngrok-7.2.4 starlette-0.46.2 uvicorn-0.34.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# !pip install flask flask-cors pyngrok\n!ngrok config add-authtoken 2tMCQI97LXFMzbJhFPf2oSJdlYj_3Ak5CGcaGJoUPAKVKRx7w","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:16:54.732054Z","iopub.execute_input":"2025-04-19T09:16:54.732299Z","iopub.status.idle":"2025-04-19T09:16:57.617722Z","shell.execute_reply.started":"2025-04-19T09:16:54.732278Z","shell.execute_reply":"2025-04-19T09:16:57.616561Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:16:57.619305Z","iopub.execute_input":"2025-04-19T09:16:57.619628Z","iopub.status.idle":"2025-04-19T09:17:20.820146Z","shell.execute_reply.started":"2025-04-19T09:16:57.619598Z","shell.execute_reply":"2025-04-19T09:17:20.819468Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport os\nimport re\nimport uvicorn\nimport nest_asyncio\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom fastapi import FastAPI, HTTPException\nfrom pyngrok import ngrok\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:17:20.821021Z","iopub.execute_input":"2025-04-19T09:17:20.821678Z","iopub.status.idle":"2025-04-19T09:17:21.360121Z","shell.execute_reply.started":"2025-04-19T09:17:20.821635Z","shell.execute_reply":"2025-04-19T09:17:21.359425Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# **************** Vector DB ****************","metadata":{}},{"cell_type":"code","source":"# Load pre-trained embedding model\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Load textbook passages (assuming a CSV file)\ndf = pd.read_csv(\"/kaggle/input/passag-topic/passage-topic.csv\")  # Columns: [\"passage\", \"topic\"]\n\n# Generate embeddings\nembeddings = embedding_model.encode(df[\"Passage\"].tolist(), normalize_embeddings=True)\n\n# Initialize FAISS vector database\ndimension = embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(np.array(embeddings))\n\n# Save the index for future use\nfaiss.write_index(index, \"textbook_index.faiss\")\n\n# Save metadata (passages & topics)\ndf.to_csv(\"metadata.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:17:21.360803Z","iopub.execute_input":"2025-04-19T09:17:21.361097Z","iopub.status.idle":"2025-04-19T09:17:31.911217Z","shell.execute_reply.started":"2025-04-19T09:17:21.361075Z","shell.execute_reply":"2025-04-19T09:17:31.910088Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cb1c89bf09e4b7db0b361e8aa75f9a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99998bcb52354a65876d646a4c9ff04c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ab1317bdb504c9cbe9395855b87f793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20eb753abef84afebc2f217d5964b1dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78381f23a3364c1896b80d12811176b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4aeacd8ca2b425b8ec54fce6d9e60a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95b6c54419e64efea85f78f0f59b526a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48c361c11c74486794d33f8356167138"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47fed23df93747008294f917fdc8d7cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bc58caf1b914331b05cbf5e367097cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38175d320a354fecb61aab61b05c8851"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7608559e2b294ff3b0be7a3184a94780"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## **************** Passage Retriever ****************","metadata":{}},{"cell_type":"code","source":"def retrieve_passage(query, top_k=1):\n    # print(\"retrieve_passage\")\n    # Load FAISS index and metadata\n    index = faiss.read_index(\"textbook_index.faiss\")\n    df = pd.read_csv(\"metadata.csv\")\n\n    # Encode user query\n    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n\n    # Retrieve most relevant passages\n    _, indices = index.search(np.array(query_embedding), top_k)\n    retrieved_passages = df.iloc[indices[0]][\"Passage\"].tolist()\n\n    return retrieved_passages[0]  # Return the best match\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:17:31.912214Z","iopub.execute_input":"2025-04-19T09:17:31.912535Z","iopub.status.idle":"2025-04-19T09:17:31.917480Z","shell.execute_reply.started":"2025-04-19T09:17:31.912500Z","shell.execute_reply":"2025-04-19T09:17:31.916511Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## **************** MCQ Generator ****************","metadata":{}},{"cell_type":"code","source":"# # Load Model\n# model_name = \"goenkalokesh/MCQGenerator\"  # Your Hugging Face model repo\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# # Check device compatibility\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     model_name, \n#     torch_dtype=dtype,  \n#     device_map=\"auto\"\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:17:31.919580Z","iopub.execute_input":"2025-04-19T09:17:31.919890Z","iopub.status.idle":"2025-04-19T09:17:31.937316Z","shell.execute_reply.started":"2025-04-19T09:17:31.919840Z","shell.execute_reply":"2025-04-19T09:17:31.936442Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# %%%%%%%%%%%%%%%%%%%%%%%","metadata":{}},{"cell_type":"code","source":"from transformers import LlamaForCausalLM, BitsAndBytesConfig, AutoTokenizer\nfrom peft import PeftModel\n\ntokenizer_base = AutoTokenizer.from_pretrained(\"/kaggle/input/llama-3.1/transformers/8b-instruct/2\")\n\n# Define a BitsAndBytesConfig for 4-bit quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",  # Choose \"nf4\" for better precision\n    bnb_4bit_use_double_quant=True,  # Optional: Use double quantization\n    bnb_4bit_compute_dtype=\"float16\"  # Set compute to float16 to save memory\n)\n\n\n# finetuned_model_path = \"/kaggle/input/llama_16-1/transformers/v1/1\"  \nfinetuned_model_path = \"/kaggle/input/rephrased_llama_3.1/transformers/v1/1\"  \n\n# Load the tokenizer used for fine-tuning\n# tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\ntokenizer = tokenizer_base\nprint(\"0\")\n# Load the model with quantization config\nmodel = LlamaForCausalLM.from_pretrained(\n    finetuned_model_path,\n    ignore_mismatched_sizes=True,\n    quantization_config=quantization_config,\n    device_map=\"auto\" \n)\nprint(\"1\")\n\n# Now load the LoRA adapter (if needed)\nlora_adapter_path = \"/kaggle/input/rephrased_llama_3.1/transformers/v1/1/lora\"\ntuned_model = PeftModel.from_pretrained(model, lora_adapter_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:17:31.938403Z","iopub.execute_input":"2025-04-19T09:17:31.938733Z","iopub.status.idle":"2025-04-19T09:18:00.920870Z","shell.execute_reply.started":"2025-04-19T09:17:31.938712Z","shell.execute_reply":"2025-04-19T09:18:00.919932Z"}},"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f8bd8160024004aa172d35e3f5955a"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a57650c3c204a798d6e4ed1c10f03fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"881d041d3b334d34a22e7b3c2c667d86"}},"metadata":{}},{"name":"stdout","text":"1\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def MCQ_generation(level, no_mcq,topic):\n    # print(\"MCQ_generation\")\n    passage = retrieve_passage(topic)\n    # passage = topic\n    prompt = f\"\"\"\n            You are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments.\n            Your goal is to generate exactly {no_mcq} clear, well-structured MCQs with four answer options each.\n            Ensure each MCQ has options labeled A, B, C, and D.\n            Also, give the answer key for all the questions at the end.\n            \n            Do not generate more or fewer than {no_mcq} questions.\n            Do not include explanations or extra commentary.\n            \n            ### Instruction:\n            Generate exactly {no_mcq} {level}-level MCQs based on the following passage:\n            \n            ### Input:\n            {passage}\n            \n            ### Response:\n            \"\"\"   \n        \n    input_text = prompt\n    # input_text = evaluation_data[18]\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    \n    # Generate text\n    with torch.no_grad():\n        outputs = tuned_model.generate(**inputs, max_length=2048)  \n    \n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # print(generated_text)\n    return generated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:18:00.922187Z","iopub.execute_input":"2025-04-19T09:18:00.922499Z","iopub.status.idle":"2025-04-19T09:18:00.927893Z","shell.execute_reply.started":"2025-04-19T09:18:00.922478Z","shell.execute_reply":"2025-04-19T09:18:00.927096Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import LlamaForCausalLM, BitsAndBytesConfig, AutoTokenizer\nfrom peft import PeftModel\n\n# tokenizer_base = AutoTokenizer.from_pretrained(\"/kaggle/input/llama-3.1/transformers/8b-instruct/2\")\n\n# Define a BitsAndBytesConfig for 4-bit quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",  # Choose \"nf4\" for better precision\n    bnb_4bit_use_double_quant=True,  # Optional: Use double quantization\n    bnb_4bit_compute_dtype=\"float16\"  # Set compute to float16 to save memory\n)\n\n\nfinetuned_model_path = \"/kaggle/input/llama_subtopic/transformers/default/1\"  \n\n\n# Load the tokenizer used for fine-tuning\n# tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\ntokenizer = tokenizer_base\nprint(\"0\")\n# Load the model with quantization config\nmodel = LlamaForCausalLM.from_pretrained(\n    finetuned_model_path,\n    ignore_mismatched_sizes=True,\n    quantization_config=quantization_config,\n    device_map=\"auto\" \n)\nprint(\"1\")\n\n# Now load the LoRA adapter (if needed)\nlora_adapter_path = \"/kaggle/input/llama_subtopic/transformers/default/1/lora\"\nsubTopic_model = PeftModel.from_pretrained(model, lora_adapter_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:18:00.928657Z","iopub.execute_input":"2025-04-19T09:18:00.928983Z","iopub.status.idle":"2025-04-19T09:18:19.384479Z","shell.execute_reply.started":"2025-04-19T09:18:00.928962Z","shell.execute_reply":"2025-04-19T09:18:19.383737Z"}},"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"name":"stdout","text":"1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def subtopic_generation(mcq, topic):\n    # print(\"MCQ_generation\")\n    # passage = retrieve_passage(topic)\n    prompt = f\"\"\"\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {mcq}\n            Topic: {topic}\n            \n            ### Response:\n            Sub-topic:\n            \"\"\"\n        \n    input_text = prompt\n    # input_text = evaluation_data[18]\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    \n    # Generate text\n    with torch.no_grad():\n        outputs = subTopic_model.generate(**inputs, max_length=2048)  \n    \n    sub_topic = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(sub_topic)\n    return sub_topic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:18:55.297110Z","iopub.execute_input":"2025-04-19T09:18:55.297489Z","iopub.status.idle":"2025-04-19T09:18:55.302335Z","shell.execute_reply.started":"2025-04-19T09:18:55.297457Z","shell.execute_reply":"2025-04-19T09:18:55.301411Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%","metadata":{}},{"cell_type":"code","source":" # prompt = f\"\"\"\n #        You are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments.\n #        Your goal is to generate clear, well-structured MCQs with answers.\n #        Ensure each MCQ has four answer options. Also, give the answer key for all the questions in the end.\n #        Remember not to give anything extra after the answer key.\n                \n #        ### Instruction:\n #        Generate {no_mcq} {level}-level MCQs based on the following passage:\n        \n #        ### Input:\n #        {passage}\n\n        \n #        ### Response:\n #        \"\"\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T01:25:56.695035Z","iopub.execute_input":"2025-04-12T01:25:56.695367Z","iopub.status.idle":"2025-04-12T01:25:56.717421Z","shell.execute_reply.started":"2025-04-12T01:25:56.695335Z","shell.execute_reply":"2025-04-12T01:25:56.716372Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# def MCQ_generation(level, no_mcq,topic):\n#     # print(\"MCQ_generation\")\n#     passage = retrieve_passage(topic)\n#     # passage = topic\n#     prompt = f\"\"\"\n#         You are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments.\n#         Your goal is to generate clear, well-structured MCQs with answers.\n#         Ensure each MCQ has four answer options. Also, give the answer key for all the questions in the end.\n#         Remember not to give anything extra after the answer key.\n                \n#         ### Instruction:\n#         Generate {no_mcq} {level}-level MCQs based on the following passage:\n        \n#         ### Input:\n#         {passage}\n    \n        \n#         ### Response:\n#         \"\"\" \n   \n        \n#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)  # Move inputs to correct device\n#     outputs = model.generate(**inputs, max_length=2048)\n#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n#     # inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    \n#     # # Generate text\n#     # with torch.no_grad():\n#     #     outputs = tuned_model.generate(**inputs, max_length=2048)  \n    \n#     # generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n#     # print(generated_text)\n#     return response\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T01:25:56.718459Z","iopub.execute_input":"2025-04-12T01:25:56.718788Z","iopub.status.idle":"2025-04-12T01:25:56.734575Z","shell.execute_reply.started":"2025-04-12T01:25:56.718756Z","shell.execute_reply":"2025-04-12T01:25:56.733511Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## **************** MCQ Extractor ****************","metadata":{}},{"cell_type":"code","source":"# def extract_mcqs_and_answers(text):\n#     # Get the response section\n#     mcq_section = re.split(r'Response:\\s*\\n?', text, maxsplit=1)[-1].strip()\n\n#     # Extract answer keys (A, B, C, D on separate lines at the end)\n#     answer_keys = re.findall(r'(?m)^[A-D]$', mcq_section.strip())\n    \n#     # Remove answer key lines from the bottom\n#     if answer_keys:\n#         mcq_text = mcq_section.rsplit('\\n', len(answer_keys))[0].strip()\n#     else:\n#         mcq_text = mcq_section\n\n#     # Split MCQs using \"Q1:\", \"Q2:\", etc. as the separator\n#     raw_mcqs = re.split(r'(?=Q\\d+:)', mcq_text)\n    \n#     # Clean and filter out empty strings\n#     mcqs = [mcq.strip() for mcq in raw_mcqs if mcq.strip()]\n\n#     return mcqs, answer_keys","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:18:56.849465Z","iopub.execute_input":"2025-04-19T09:18:56.849820Z","iopub.status.idle":"2025-04-19T09:18:56.853424Z","shell.execute_reply.started":"2025-04-19T09:18:56.849793Z","shell.execute_reply":"2025-04-19T09:18:56.852570Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def extract_mcqs_and_answers(text):\n    # Get everything after 'Response:'\n    mcq_section = re.split(r'Response:\\s*\\n?', text, maxsplit=1)[-1].strip()\n\n    # Split into lines\n    lines = mcq_section.strip().split('\\n')\n\n    answer_keys = []\n    mcq_lines = []\n\n    # Start from bottom, collect A/B/C/D lines\n    for line in reversed(lines):\n        if re.fullmatch(r'[A-D]', line.strip()):\n            answer_keys.insert(0, line.strip())  # Add to front to keep order\n        else:\n            break\n\n    # The remaining lines are the MCQs\n    mcq_lines = lines[:len(lines) - len(answer_keys)]\n\n    # Join and split MCQs\n    mcq_text = '\\n'.join(mcq_lines)\n    raw_mcqs = re.split(r'(?=Q\\d+:)', mcq_text)\n\n    mcqs = [q.strip() for q in raw_mcqs if q.strip()]\n\n    return mcqs, answer_keys\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:18:57.048477Z","iopub.execute_input":"2025-04-19T09:18:57.048820Z","iopub.status.idle":"2025-04-19T09:18:57.054738Z","shell.execute_reply.started":"2025-04-19T09:18:57.048788Z","shell.execute_reply":"2025-04-19T09:18:57.053734Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!pip install python-multipart","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:18:58.280606Z","iopub.execute_input":"2025-04-19T09:18:58.280932Z","iopub.status.idle":"2025-04-19T09:19:01.954869Z","shell.execute_reply.started":"2025-04-19T09:18:58.280906Z","shell.execute_reply":"2025-04-19T09:19:01.953974Z"}},"outputs":[{"name":"stdout","text":"Collecting python-multipart\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nInstalling collected packages: python-multipart\nSuccessfully installed python-multipart-0.0.20\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Initialize FastAPI app\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # For testing\n    # allow_origins=[\"https://pla-frontend.vercel.app\"], # for deploymnet\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nuser_state = {\n    \"incorrect_questions\": [],\n    \"incorrect_Sub_topics\": []\n}\n\nscore_state = {\n    \"main\": {\n        \"Easy\": {\"correct\": 0, \"attempted\": 0},\n        \"Medium\": {\"correct\": 0, \"attempted\": 0},\n        \"Hard\": {\"correct\": 0, \"attempted\": 0}\n    },\n    \"remedial\": {\n        \"Easy\": {\"correct\": 0, \"attempted\": 0},\n        \"Medium\": {\"correct\": 0, \"attempted\": 0},\n        \"Hard\": {\"correct\": 0, \"attempted\": 0}\n    }\n}\n\nincorrect_questions = []\n\n# Load Model\n# MODEL_NAME = \"goenkalokesh/MCQGenerator\"  # Your Hugging Face model\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# # Device selection\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     MODEL_NAME, \n#     torch_dtype=dtype,  \n#     device_map=\"auto\"\n# )\n\n# ---------------------------------------\n# ğŸš€ Utility Functions\n# ---------------------------------------\n\ndef generate_mcqs(topic, level, num_questions):\n    \"\"\"Generate MCQs based on topic and level with retries.\"\"\"\n    response = MCQ_generation(level, num_questions, topic)\n    # print(response)\n    mcq_pattern, answer_key = extract_mcqs_and_answers(response)\n    # print(mcq_pattern)\n    return [{\"question\": q, \"answer\": a} for q, a in zip(mcq_pattern, answer_key)]\n    # for _ in range(5):  # Retry up to 5 times\n    #     response = MCQ_generation(level, num_questions, topic)\n    #     mcq_pattern, answer_key = extract_mcqs_and_answers(response)\n\n    #     if len(mcq_pattern) & len(answer_key) in [2, 3]:  # Ensure valid output\n    #         return [{\"question\": q, \"answer\": a} for q, a in zip(mcq_pattern, answer_key)]\n    \n    # return {\"error\": \"Failed to generate valid MCQs after multiple attempts.\"}\n\ndef generate_sub_topic(incorrect_mcqs, topic):\n    sub_topics = []\n    for mcq in incorrect_mcqs:\n        response = subtopic_generation(mcq, topic)\n        sub_topic = re.split(r'Response:\\n', response, maxsplit=1)[-1].strip()\n        sub_topics.append(sub_topic)\n    return sub_topics\n\n\n\n# ---------------------------------------\n# âœ… API Endpoints\n# ---------------------------------------\n\n@app.get(\"/\")\ndef home():\n    return {\"message\": \"API is running successfully!\"}\n\n@app.get(\"/start-test/\")\ndef start_test(topic: str):\n    \"\"\"Start an MCQ-based test on a given topic.\"\"\"\n    \n    # Begin with Easy Level\n    level = \"Easy\"\n    mcqs = generate_mcqs(topic, level, num_questions=3)\n    return {\"level\": level, \"questions\": mcqs}\n\n@app.post(\"/submit-answers/\")\ndef submit_answers(data: dict):\n    question_type = data.get(\"question_type\")  # \"Main\" or \"Remedial\"\n    level = data.get(\"level\")\n    topic = data.get(\"topic\")\n    questions = data.get(\"questions\", [])\n    user_answers = data.get(\"answers\", [])\n    correct_answers = data.get(\"correct_answers\", [])\n\n    # Tally correct/incorrect\n    correct_count = sum(1 for i in range(len(questions)) if user_answers[i] == correct_answers[i])\n    incorrect_count = len(questions) - correct_count\n    attempted_count = len(questions)\n\n    # Track incorrect questions\n    for i in range(len(questions)):\n        if user_answers[i] != correct_answers[i]:\n            user_state[\"incorrect_questions\"].append(questions[i])\n\n    # Update score\n    score_state[question_type.lower()][level][\"correct\"] += correct_count\n    score_state[question_type.lower()][level][\"attempted\"] += attempted_count\n\n    total_correct = (\n        score_state[\"main\"][level][\"correct\"] + score_state[\"remedial\"][level][\"correct\"]\n    )\n    total_attempted = (\n        score_state[\"main\"][level][\"attempted\"] + score_state[\"remedial\"][level][\"attempted\"]\n    )\n\n    # -----------------------------------------\n    # ğŸ“Œ Handle \"Main\" Question Type Logic\n    # -----------------------------------------\n    if question_type == \"Main\":\n        if correct_count == 3:\n            # âœ… All correct â†’ move to next level\n            next_level = {\"Easy\": \"Medium\", \"Medium\": \"Hard\", \"Hard\": \"Completed\"}[level]\n            return {\"message\": \"Next Level\", \"next_level\": next_level}\n        elif incorrect_count == 3:\n            # âŒ All incorrect â†’ STOP test\n            incorrect_sub_topics = generate_sub_topic(user_state[\"incorrect_questions\"], topic)\n            user_state[\"incorrect_Sub_topics\"].extend(incorrect_sub_topics)\n            return {\n                \"message\": \"You need more practice\",\n                \"retry\": False,\n                \"Sub_topics\": incorrect_sub_topics\n            }\n        else:\n            # ğŸ› ï¸ Mixed â†’ remedial round\n            remedial_mcqs = generate_mcqs(topic, level, num_questions=2)\n            return {\"message\": \"Remedial Questions\", \"questions\": remedial_mcqs}\n\n    # -----------------------------------------\n    # ğŸ“Œ Handle \"Remedial\" Question Type Logic\n    # -----------------------------------------\n    elif question_type == \"Remedial\":\n        if total_correct >= 3:\n            # âœ… Move to next level\n            next_level = {\"Easy\": \"Medium\", \"Medium\": \"Hard\", \"Hard\": \"Completed\"}[level]\n            if next_level == \"Completed\":\n                # Test finished â†’ calculate final score\n                level_weights = {\"Easy\": 1, \"Medium\": 3, \"Hard\": 5}\n                remedial_weights = {\"Easy\": 0.5, \"Medium\": 2.5, \"Hard\": 4.5}\n\n                numerator = sum(\n                    score_state[\"main\"][l][\"correct\"] * level_weights[l]\n                    for l in score_state[\"main\"]\n                ) + sum(\n                    score_state[\"remedial\"][l][\"correct\"] * remedial_weights[l]\n                    for l in score_state[\"remedial\"]\n                )\n\n                denominator = sum(\n                    score_state[\"main\"][l][\"attempted\"] * level_weights[l]\n                    for l in score_state[\"main\"]\n                ) + sum(\n                    score_state[\"remedial\"][l][\"attempted\"] * remedial_weights[l]\n                    for l in score_state[\"remedial\"]\n                )\n\n                percentage = (numerator / denominator) * 100 if denominator else 0\n\n                incorrect_sub_topics = generate_sub_topic(user_state[\"incorrect_questions\"], topic)\n                user_state[\"incorrect_Sub_topics\"].extend(incorrect_sub_topics)\n\n                return {\n                    \"message\": \"Test completed! Well done!\",\n                    \"score\": round(percentage, 2),\n                    \"Sub_topics\": incorrect_sub_topics\n                }\n            \n            return {\"message\": \"Next Level\", \"next_level\": next_level}\n\n        else:\n            # âŒ Total < 3 correct â†’ Stop test\n            incorrect_sub_topics = generate_sub_topic(user_state[\"incorrect_questions\"], topic)\n            user_state[\"incorrect_Sub_topics\"].extend(incorrect_sub_topics)\n            return {\n                \"message\": \"You need more practice\",\n                \"retry\": False,\n                \"Sub_topics\": incorrect_sub_topics\n            }\n\n\n@app.get(\"/final-score/\")\ndef final_score():\n    level_weights = {\"Easy\": 1, \"Medium\": 3, \"Hard\": 5}\n    remedial_weights = {\"Easy\": 0.5, \"Medium\": 2.5, \"Hard\": 4.5}\n\n    numerator = 0\n    denominator = 0\n\n    for level in level_weights:\n        main_correct = score_state[\"main\"][level][\"correct\"]\n        main_attempted = score_state[\"main\"][level][\"attempted\"]\n        remedial_correct = score_state[\"remedial\"][level][\"correct\"]\n        remedial_attempted = score_state[\"remedial\"][level][\"attempted\"]\n\n        numerator += (main_correct * level_weights[level]) + (remedial_correct * remedial_weights[level])\n        denominator += (main_attempted * level_weights[level]) + (remedial_attempted * remedial_weights[level])\n\n    percentage = (numerator / denominator) * 100 if denominator > 0 else 0\n\n    result = {\n        \"score_percentage\": round(percentage, 2),\n        \"details\": score_state\n    }\n\n    # Reset after evaluation\n    for qtype in score_state:\n        for level in score_state[qtype]:\n            score_state[qtype][level][\"correct\"] = 0\n            score_state[qtype][level][\"attempted\"] = 0\n\n    return result\n\n\n\n# ---------------------------------------\n# ğŸš€ Run the API\n# ---------------------------------------\n\nnest_asyncio.apply()\n\nif __name__ == \"__main__\":\n    import uvicorn\n    from pyngrok import ngrok\n\n    # Set your Ngrok auth token\n    # ngrok.set_auth_token(\"your-ngrok-auth-token\")  # or use os.getenv if it's in env\n\n    # Kill any previous tunnels (optional but clean)\n    ngrok.kill()\n\n    # Connect to your reserved domain\n    tunnel = ngrok.connect(addr=8000, domain=\"quiet-safely-pegasus.ngrok-free.app\")\n    print(f\"API running at {tunnel.public_url}\")\n\n    # Start the FastAPI app\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:19:01.956348Z","iopub.execute_input":"2025-04-19T09:19:01.956597Z","execution_failed":"2025-04-19T09:30:06.114Z"}},"outputs":[{"name":"stderr","text":"INFO:     Started server process [31]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"API running at https://quiet-safely-pegasus.ngrok-free.app\nINFO:     136.232.201.86:0 - \"OPTIONS /start-test/?topic=World%20War%20I&level=Easy HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33fa95149d514629960dfa1f96f42919"}},"metadata":{}},{"name":"stdout","text":"INFO:     136.232.201.86:0 - \"GET /start-test/?topic=World%20War%20I&level=Easy HTTP/1.1\" 200 OK\nINFO:     136.232.201.86:0 - \"OPTIONS /submit-answers/ HTTP/1.1\" 200 OK\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q1: What was the main reason for the prolonged economic crisis in Britain after the First World War?\\n (A) High war expenditures\\n (B) Loss of colonies\\n (C) Decline of the manufacturing sector\\n (D) All of the above', 'answer': 'D'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Economic Crisis\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q2: How did the war impact the agricultural sector?\\n (A) It led to a decline in wheat production\\n (B) It increased the demand for agricultural products\\n (C) It had no impact on agriculture\\n (D) It led to an increase in the price of agricultural products', 'answer': 'A'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             The impact of World War I on agriculture.\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': \"Q3: What was the significance of Henry Ford's assembly line?\\n (A) It increased the speed and efficiency of production\\n (B) It reduced the cost of production\\n (C) It improved the working conditions of workers\\n (D) All of the above\", 'answer': 'A'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Ford Assembly Line\n\nINFO:     136.232.201.86:0 - \"POST /submit-answers/ HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eb5cf9cd9444c1eb59dd6a84459ec41"}},"metadata":{}},{"name":"stdout","text":"INFO:     136.232.201.86:0 - \"GET /start-test/?topic=World%20War%20I&level=Easy HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f31d50308ca486e8ace15135c411559"}},"metadata":{}},{"name":"stdout","text":"INFO:     136.232.201.86:0 - \"POST /submit-answers/ HTTP/1.1\" 200 OK\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q1: What was the main reason for the prolonged economic crisis in Britain after the First World War?\\n (A) High war expenditures\\n (B) Loss of colonies\\n (C) Decline of the manufacturing sector\\n (D) All of the above', 'answer': 'D'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Economic Crisis in Britain\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q2: How did the war impact the agricultural sector?\\n (A) It led to a decline in wheat production\\n (B) It increased the demand for agricultural products\\n (C) It had no impact on agriculture\\n (D) It led to an increase in the price of agricultural products', 'answer': 'A'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Agriculture Impact\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': \"Q3: What was the significance of Henry Ford's assembly line?\\n (A) It increased the speed and efficiency of production\\n (B) It reduced the cost of production\\n (C) It improved the working conditions of workers\\n (D) All of the above\", 'answer': 'A'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Mass Production and Its Role in Economic Growth\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q1: What was the main reason for the prolonged economic crisis in Britain after the First World War?\\n (A) It had borrowed too much money from the US\\n (B) Its industries had been destroyed during the war\\n (C) Its colonies had gained independence\\n (D) It had lost its colonies', 'answer': 'A'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             The Economic Crisis in Britain\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q2: How did the war affect the wheat industry in eastern Europe?\\n (A) It led to a decline in production\\n (B) It caused a glut in the market\\n (C) It led to an increase in prices\\n (D) It had no impact on the industry', 'answer': 'B'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Social and Economic Consequences\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q4: How did the war affect the agricultural sector?\\n A) It led to a decline in wheat production\\n B) It increased the demand for agricultural products\\n C) It had no significant impact on agriculture\\n D) It led to an increase in the production of wheat', 'answer': 'A'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             The MCQ is related to the social and economic consequences of World War I, which is a sub-topic within the broader topic of the war.\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q5: What was the main characteristic of mass production in the US in the 1920s?\\n A) It used skilled labor\\n B) It was based on the assembly line\\n C) It produced only consumer goods\\n D) It led to a decline in wages', 'answer': 'B'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Global Economic Consequences\n\nINFO:     136.232.201.86:0 - \"POST /submit-answers/ HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed3e9a5dc22f4152a60f9aefe484cd6b"}},"metadata":{}},{"name":"stdout","text":"INFO:     136.232.201.86:0 - \"GET /start-test/?topic=World%20War%20I&level=Easy HTTP/1.1\" 200 OK\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q1: What was the main reason for the prolonged economic crisis in Britain after the First World War?\\n (A) High war expenditures\\n (B) Loss of colonies\\n (C) Decline of the manufacturing sector\\n (D) All of the above', 'answer': 'D'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Economic Crisis\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q2: How did the war impact the agricultural sector?\\n (A) It led to a decline in wheat production\\n (B) It increased the demand for agricultural products\\n (C) It had no impact on agriculture\\n (D) It led to an increase in the price of agricultural products', 'answer': 'A'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Agriculture and WWI\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': \"Q3: What was the significance of Henry Ford's assembly line?\\n (A) It increased the speed and efficiency of production\\n (B) It reduced the cost of production\\n (C) It improved the working conditions of workers\\n (D) All of the above\", 'answer': 'A'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             The assembly line increased the speed and efficiency of production, reduced the cost of production, and improved the working conditions of workers.\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q1: What was the main reason for the prolonged economic crisis in Britain after the First World War?\\n (A) It had borrowed too much money from the US\\n (B) Its industries had been destroyed during the war\\n (C) Its colonies had gained independence\\n (D) It had lost its colonies', 'answer': 'A'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             The Economic Consequences of the War\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q2: How did the war affect the wheat industry in eastern Europe?\\n (A) It led to a decline in production\\n (B) It caused a glut in the market\\n (C) It led to an increase in prices\\n (D) It had no impact on the industry', 'answer': 'B'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Social and Economic Consequences\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q4: How did the war affect the agricultural sector?\\n A) It led to a decline in wheat production\\n B) It increased the demand for agricultural products\\n C) It had no significant impact on agriculture\\n D) It led to an increase in the production of wheat', 'answer': 'A'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Agriculture\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q5: What was the main characteristic of mass production in the US in the 1920s?\\n A) It used skilled labor\\n B) It was based on the assembly line\\n C) It produced only consumer goods\\n D) It led to a decline in wages', 'answer': 'B'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Mass Production\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q1: What was the main reason for the outbreak of World War I in 1914?\\n (A) Imperialism\\n (B) Nationalism\\n (C) Economic competition\\n (D) Religious differences', 'answer': 'C'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Causes of World War I\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q2: Which of the following was NOT a major industrial product used in World War I?\\n (A) Machine guns\\n (B) Tanks\\n (C) Aircraft\\n (D) Chemical weapons', 'answer': 'D'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             â€¢ Causes of World War I\n             â€¢ Role of Nationalism\n             â€¢ Assassination of Archduke Franz Ferdinand\n             â€¢ Alliance Systems\n             â€¢ Social and Economic Consequences\n\n\n            ### Instruction:\n            Given a multiple-choice question (MCQ) and its associated topic, predict the sub-topic it belongs to.\n            \n            ### Input:\n            MCQ: {'question': 'Q3: What was the main cause of the economic boom in the US during World War I?\\n (A) Government spending\\n (B) Increased exports\\n (C) Mass production\\n (D) Immigration', 'answer': 'C'}\n            Topic: World War I\n            \n            ### Response:\n            Sub-topic:\n             Economic Boom\n\nINFO:     136.232.201.86:0 - \"POST /submit-answers/ HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Logic in backend","metadata":{}},{"cell_type":"code","source":"# from fastapi import FastAPI, HTTPException, Query\n# from pydantic import BaseModel\n# import random\n# import os\n# import nest_asyncio\n# from pyngrok import ngrok\n# import uvicorn\n# from fastapi.middleware.cors import CORSMiddleware\n\n# app = FastAPI()\n\n# app.add_middleware(\n#     CORSMiddleware,\n#     allow_origins=[\"*\"],  # Allow all origins (You can restrict this to your frontend domain)\n#     allow_credentials=True,\n#     allow_methods=[\"*\"],  # Allow all methods (GET, POST, OPTIONS, etc.)\n#     allow_headers=[\"*\"],  # Allow all headers\n# )\n\n# total_levels = [\"Easy\", \"Medium\", \"Hard\"]  \n# level_weights = {\"Easy\": 1, \"Medium\": 3, \"Hard\": 5}  \n# remedial_weights = {\"Easy\": 0.5, \"Medium\": 2.5, \"Hard\": 4.5}  \n\n# # Mock MCQ generation function (replace with actual logic)\n# def generate_mcqs(level, num_questions, topic):\n#     questions = []\n#     for i in range(num_questions):\n#         question = {\n#             \"id\": f\"{level}_{i}\",\n#             \"question\": f\"Sample question {i+1} for {topic} [{level}]?\",\n#             \"options\": [\"A\", \"B\", \"C\", \"D\"],\n#             \"answer\": random.choice([\"A\", \"B\", \"C\", \"D\"]),\n#         }\n#         questions.append(question)\n#     return questions\n\n# class AnswerSubmission(BaseModel):\n#     level: str\n#     user_answers: dict  # {\"question_id\": \"User's Answer\"}\n#     topic: str\n\n# user_progress = {\"level\": \"Easy\", \"correct\": 0, \"incorrect\": 0, \"incorrect_qs\": [], \"correct_qs\": []}\n\n# @app.post(\"/start-test\")\n# def start_test(topic: str = Query(...)):\n#     mcqs = generate_mcqs(\"Easy\", 3, topic)\n#     return {\"level\": \"Easy\", \"mcqs\": mcqs}\n\n# @app.post(\"/submit-answers\")\n# def submit_answers(submission: AnswerSubmission):\n#     level = submission.level\n#     user_answers = submission.user_answers\n#     topic = submission.topic\n    \n#     correct_count = sum(1 for q_id, ans in user_answers.items() if ans == \"A\")  # Mock answer checking\n#     incorrect_count = len(user_answers) - correct_count\n    \n#     user_progress[\"correct\"] += correct_count\n#     user_progress[\"incorrect\"] += incorrect_count\n    \n#     if incorrect_count >= 3:\n#         return {\"status\": \"failed\", \"message\": f\"User failed {level} level.\", \"incorrect_questions\": user_progress[\"incorrect_qs\"]}\n    \n#     if correct_count >= 3:\n#         next_level_index = total_levels.index(level) + 1\n#         if next_level_index < len(total_levels):\n#             next_level = total_levels[next_level_index]\n#             mcqs = generate_mcqs(next_level, 3, topic)\n#             user_progress[\"level\"] = next_level\n#             return {\"status\": \"next_level\", \"level\": next_level, \"mcqs\": mcqs}\n#         else:\n#             return {\"status\": \"completed\", \"message\": \"Test completed successfully!\"}\n    \n#     mcqs = generate_mcqs(level, 2, topic)\n#     return {\"status\": \"remedial\", \"level\": level, \"mcqs\": mcqs}\n\n# # ---------------------------------------\n# # ğŸš€ Run the API\n# # ---------------------------------------\n\n# nest_asyncio.apply()\n\n# if __name__ == \"__main__\":\n#     # Get NGROK Token from environment\n#     NGROK_AUTH_TOKEN = os.getenv(\"NGROK_AUTH_TOKEN\")\n#     if NGROK_AUTH_TOKEN:\n#         ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n    \n#     public_url = ngrok.connect(8000).public_url\n#     print(f\"API running at {public_url}\")\n\n#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T01:29:00.688Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}